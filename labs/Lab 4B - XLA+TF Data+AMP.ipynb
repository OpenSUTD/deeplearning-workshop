{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 4B - XLA+TF.Data+AMP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "h7R_FcIPA1Ko"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7R_FcIPA1Ko",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTqhOE9jWQRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu>=1.14 gpustat -qU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOug94VRWWd0",
        "colab_type": "code",
        "outputId": "31af9f59-77b1-4858-e6c8-0fa38c45c6c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0820 08:12:13.538593 139768746674048 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEi36ou7KySx",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYijcApeY5R4",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we are going to be doing image classification on the [Kaggle \"Cats vs Dogs\" dataset](https://www.kaggle.com/c/dogs-vs-cats).\n",
        "\n",
        "However, we are not going to be training the model from scratch. Instead, we will be leveraging **transfer learning** to reduce the amount of time needed to train a very high accuracy model on a relatively small dataset. Transfer learning works by taking a model that has been trained on a much larger dataset (e.g. ImageNet), and then fine-tuning it for another task. This works because much of the model learns features that apply to the same type images in general (e.g. RGB photographs). \n",
        "\n",
        "This notebook demonstrates a typical example of transfer learning where we take a model that has been trained on ImageNet, and fine-tune it for our own task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEiB77wqK1vy",
        "colab_type": "code",
        "outputId": "293438fc-5864-4c3b-d904-d3989b463b29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!gpustat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[37mf5e523abe720       \u001b[m  Tue Aug 20 08:12:14 2019  \u001b[1m\u001b[30m410.79\u001b[m\n",
            "\u001b[36m[0]\u001b[m \u001b[34mTesla T4        \u001b[m |\u001b[1m\u001b[31m 69'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    0\u001b[m / \u001b[33m15079\u001b[m MB |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZieFWCpLUtO",
        "colab_type": "text"
      },
      "source": [
        "## Construct Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVgB1lohZu2l",
        "colab_type": "text"
      },
      "source": [
        "This section demonstrates how to build a high-performance input pipeline for feeding in training and evaluation data into the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4O-FO1UaYeY",
        "colab_type": "text"
      },
      "source": [
        "### Load Dataset using TensorFlow Datasets\n",
        "\n",
        "\"Cats vs Dogs\" is not a standard research dataset, and is not found in `tf.keras.datasets` like the more common MNIST or CIFAR10. Fortunately, **TensorFlow Datasets** not only contains this dataset, but also many others. \n",
        "\n",
        "When we load datasets with TensorFlow Datasets, we get [`tf.data.Dataset`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset) objects. This means we automatically take advantage of high performance `tf.data` primitives to create highly-optimized, efficient input pipelines that result in better utilization of our hardware accelerators, such as GPUs or TPUs that might otherwise be constantly \"waiting\" for data from the CPU. This is important because hardware accelerators for deep learning are so fast, the speed at which we can feed images into the accelerator is usually a significant bottleneck. \n",
        "\n",
        "Using `tf.data` is a best practice (especially for reading large datasets). You can read more about using tf.data at the following links:\n",
        "\n",
        "* https://www.tensorflow.org/beta/guide/data\n",
        "* https://www.tensorflow.org/guide/performance/datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4G-OIK6SVbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "opt_level = tf.OptimizerOptions.ON_1\n",
        "tf.enable_resource_variables()\n",
        "config.graph_options.optimizer_options.global_jit_level = opt_level\n",
        "config.graph_options.rewrite_options.auto_mixed_precision = True\n",
        "sess = tf.Session(config=config)\n",
        "tf.keras.backend.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf0vStKoaYCH",
        "colab_type": "code",
        "outputId": "0bc55655-2db8-4d6d-b8ca-0ac66486ac89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# define train-validation-test split\n",
        "splits = tfds.Split.TRAIN.subsplit(weighted=(8, 1, 1))\n",
        "\n",
        "# load dataset with corresponding split\n",
        "(raw_train, raw_validation, raw_test), info = tfds.load(\"cats_vs_dogs\",\n",
        "                                                        split=list(splits),\n",
        "                                                        with_info=True,\n",
        "                                                        as_supervised=True)\n",
        "\n",
        "num_examples = info.splits[\"train\"].num_examples\n",
        "num_train = int(num_examples * 0.8)\n",
        "num_val = int(num_examples * 0.1)\n",
        "num_test = int(num_examples * 0.1)\n",
        "\n",
        "print(\"Split:\", num_train, num_val, num_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split: 18609 2326 2326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOy7hAFtcnaW",
        "colab_type": "text"
      },
      "source": [
        "### Build `tf.data` Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ob-z76OwFQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 120\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "@tf.function\n",
        "def format_example(image, label):\n",
        "    \"\"\"\n",
        "    This function will run as part of a tf.data pipeline.\n",
        "    It is reponsible for resizing and normalizing the input images.\n",
        "    \"\"\"\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = (image/127.5) - 1\n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMD5_KhndBJK",
        "colab_type": "text"
      },
      "source": [
        "**Input Pipeline for Train Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMO6uEZPKmc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = raw_train.shuffle(4096)\n",
        "train = train.repeat(count=-1)\n",
        "train = train.map(format_example, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train = train.batch(BATCH_SIZE)\n",
        "train = train.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgvLwbffdL9-",
        "colab_type": "text"
      },
      "source": [
        "**Input Pipeline for Validation and Test Images**\n",
        "\n",
        "We remove some unnecessary steps (like shuffling) and increase the batch size for faster inferencing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPxsljSkdMKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val = raw_validation.repeat(count=-1)\n",
        "val = val.map(format_example, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "val = val.batch(BATCH_SIZE*2)\n",
        "val = val.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "test = raw_test.repeat(count=-1)\n",
        "test = test.map(format_example, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test = test.batch(BATCH_SIZE*2)\n",
        "test = test.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUB3aSlaN0iH",
        "colab_type": "text"
      },
      "source": [
        "## Construct the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h0pyAF0L_EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmMHAwp688E-",
        "colab_type": "code",
        "outputId": "1ecf820a-6193-4726-91bb-035bd3cfad48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "tf.keras.mixed_precision.experimental.set_policy('infer_float32_vars')\n",
        "\n",
        "input_layer = layers.Input(shape=(IMG_SIZE[0],IMG_SIZE[1],3,))\n",
        "\n",
        "base = ResNet50(input_tensor=input_layer, include_top=False, weights='imagenet')\n",
        "\n",
        "base.trainable = False\n",
        "\n",
        "x = base.output\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "preds = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=input_layer, outputs=preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n",
            "W0820 08:12:23.864003 139768746674048 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u90VRx_5NiS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(lr=0.001)\n",
        "\n",
        "opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, \"dynamic\")\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=opt,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpB3PSWgqTLw",
        "colab_type": "text"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zkA41zrd4YJ",
        "colab_type": "code",
        "outputId": "4e3d924a-1189-4b28-99a3-66c3e5b818a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model.fit(train, validation_data=val,\n",
        "          validation_steps=num_val//BATCH_SIZE*2,\n",
        "          steps_per_epoch=num_train//BATCH_SIZE,\n",
        "          epochs=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "155/155 [==============================] - 114s 735ms/step - loss: 0.1514 - acc: 0.9396 - val_loss: 0.7228 - val_acc: 0.5221\n",
            "Epoch 2/3\n",
            "155/155 [==============================] - 84s 541ms/step - loss: 0.0688 - acc: 0.9762 - val_loss: 0.7113 - val_acc: 0.5298\n",
            "Epoch 3/3\n",
            "155/155 [==============================] - 83s 536ms/step - loss: 0.0572 - acc: 0.9799 - val_loss: 0.7219 - val_acc: 0.5333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d92d86c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}